{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "          \n",
    "            print \"HMM creating with: \"\n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            \n",
    "            UNKid = self.M+1;  \n",
    "            \n",
    "            self.omega_Y = state_list\n",
    "            self.omega_Y.append(\"*\")\n",
    "            self.omega_X = observation_list\n",
    "            \n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N+1, self.N+1, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M+1, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "\n",
    "            self.make_indexes() \n",
    "            self.smoothing_obs = smoothing_obs \n",
    "            \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N+1):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "                \n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "      \n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs  \n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "    \n",
    "        def data2indices(self, sent): \n",
    "            \"\"\"From a word of the corpus: \n",
    "            - extract the letter and coorection \n",
    "            - returns two list of indices, one for each\n",
    "            -> (letterid, correctionid)\n",
    "            \"\"\"\n",
    "            letterids = list()\n",
    "            correctionids  = list()\n",
    "            for couple in sent:\n",
    "                ltr = couple[0]\n",
    "                crt = couple[1]\n",
    "                letterids.append(self.X_index[ltr])\n",
    "                correctionids.append(self.Y_index[crt])\n",
    "            return letterids,correctionids\n",
    "            \n",
    "        def observation_estimation(self, pair_counts):\n",
    "\n",
    "            for pair in pair_counts:\n",
    "                letter=pair[0]\n",
    "                correction=pair[1]\n",
    "                count=pair_counts[pair]\n",
    "                \n",
    "                if letter in self.X_index:\n",
    "                    k=self.X_index[letter]\n",
    "                i=self.Y_index[correction]\n",
    "                self.observation_proba[k,i]=count\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1, self.N)\n",
    "                        \n",
    "        def transition_estimation(self, c_bitag, c_tritag):\n",
    "            \n",
    "            for tritag in c_tritag:\n",
    "                #getting indices\n",
    "                y_2=self.Y_index[tritag[0]]\n",
    "                y_1=self.Y_index[tritag[1]]\n",
    "                y=self.Y_index[tritag[2]]\n",
    "                bigram=(tritag[0],tritag[1])       \n",
    "                self.transition_proba[y_2,y_1,y]=float(c_tritag[tritag])/float(c_bitag[bigram])               \n",
    "   \n",
    "        def init_estimation(self, c_inits, c_inits_bitag):\n",
    "            somme=float(sum(c_inits.values()))\n",
    "            for correction in c_inits:\n",
    "                i=self.Y_index[correction]\n",
    "                j=self.Y_index[\"*\"]\n",
    "                self.transition_proba[j,j,i]=float(c_inits[correction])/somme\n",
    "                \n",
    "            for pair in c_inits_bitag:\n",
    "                y_1=self.Y_index[pair[0]]\n",
    "                y=self.Y_index[pair[1]]\n",
    "                j=self.Y_index[\"*\"]\n",
    "                self.transition_proba[j,y_1,y]=float(c_inits_bitag[pair])/float(c_inits[pair[0]])\n",
    "                \n",
    "            \n",
    "        def supervised_training_ME(self, pair_counts, c_bitag, c_tritag ,c_inits, c_inits_bitag):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(pair_counts)\n",
    "            self.transition_estimation(c_bitag, c_tritag)\n",
    "            self.init_estimation(c_inits, c_inits_bitag)\n",
    "            \n",
    "        \n",
    "        def supervised_training_perceptron(self, T, data):\n",
    "            #Set initially parameters\n",
    "   \n",
    "            #self.transition_proba = np.zeros( (self.N+1, self.N+1, self.N), float)\n",
    "            #self.observation_proba = np.zeros( (self.M+1, self.N), float)\n",
    "\n",
    "            #self.transition_proba = np.ones( (self.N+1, self.N+1, self.N), float)\n",
    "            #self.observation_proba = np.ones( (self.M+1, self.N), float)\n",
    "    \n",
    "    \n",
    "            for iteration in range(T):\n",
    "                for observations in data:\n",
    "                    if len(observations) > 1:\n",
    "                        prob, predicted_tags = self.viterbi(observations)\n",
    "                        count_pair, count_tritag, count_tritag_predicted, count_pair_predicted = make_counts_each_observation_sequence(observations, predicted_tags)\n",
    "\n",
    "                        for tritag in count_tritag:\n",
    "                            c1 = count_tritag.get(tritag)\n",
    "                            if count_tritag_predicted.has_key(tritag):\n",
    "                                c2 = count_tritag_predicted.get(tritag)\n",
    "                            else:\n",
    "                                c2 = 0\n",
    "                    \n",
    "                            self.transition_proba[ self.Y_index[tritag[0]], self.Y_index[tritag[1]], self.Y_index[tritag[2]] ] = self.transition_proba[ self.Y_index[tritag[0]], self.Y_index[tritag[1]], self.Y_index[tritag[2]] ] + c1 - c2 \n",
    "                \n",
    "                        for pair in count_pair:\n",
    "                            c1 = count_pair.get(pair)\n",
    "                            if count_pair_predicted.has_key(pair):\n",
    "                                c2 = count_pair_predicted.get(pair)\n",
    "                            else:\n",
    "                                c2 = 0\n",
    "                    \n",
    "                            self.observation_proba[self.X_index[pair[0]],self.Y_index[pair[1]] ] = self.observation_proba[self.X_index[pair[0]],self.Y_index[pair[1]] ] + c1 - c2\n",
    "                        \n",
    "        \n",
    "        def get_possible_corrections(self,k):\n",
    "            if k == -1:\n",
    "                return set(['*'])\n",
    "            if k == 0:\n",
    "                return set(['*'])\n",
    "            else:\n",
    "                return self.omega_Y[0:26]\n",
    "\n",
    "        def get_letter(self,word,k):\n",
    "            if k < 0:\n",
    "                return '*'\n",
    "            else:\n",
    "                return word[k][0]\n",
    "    \n",
    "            \n",
    "        def viterbi(self,word):\n",
    "            \n",
    "            \n",
    "            V = {}\n",
    "            path = {}\n",
    "            # init\n",
    "            V[0,'*','*'] = 1\n",
    "            path['*','*'] = []\n",
    "            \n",
    "            for k in range(1,len(word)+1):\n",
    "                temp_path = {}\n",
    "                letter = self.X_index[self.get_letter(word,k-1)]\n",
    "                \n",
    "                for u in self.get_possible_corrections(k-1):\n",
    "                    \n",
    "                    for v in self.get_possible_corrections(k):\n",
    "\n",
    "                        i_u=self.Y_index[u]\n",
    "                        i_v=self.Y_index[v]\n",
    "                        \n",
    "                        V[k,u,v],backpointer = max([(V[k-1,w,u] * \n",
    "                                                     self.transition_proba[self.Y_index[w],i_u,i_v] * \n",
    "                                                     self.observation_proba[letter,i_v],w) \n",
    "                                                    for w in self.get_possible_corrections(k-2)])                       \n",
    "                        \n",
    "                        temp_path[u,v] = path[backpointer,u] + [v]                        \n",
    "                path = temp_path\n",
    "                 \n",
    "            prob,maxu,maxv= max([(V[k,u,v],u,v) for u in self.omega_Y[0:26] for v in self.omega_Y[0:26]])\n",
    "                \n",
    "            return prob, path[maxu,maxv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_letter: letter counts\n",
    "    * c_correction: correction counts\n",
    "    * c_pairs: count of pairs (letter,correction)\n",
    "    \n",
    "    * c_bitag: count of tag bigram \n",
    "    * c_tritag: count of tag trigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \n",
    "    \"\"\"\n",
    "    c_letter = dict()\n",
    "    c_correction = dict()\n",
    "    c_pairs= dict()\n",
    "    c_bitag = dict()\n",
    "    c_tritag = dict()\n",
    "    c_inits = dict()\n",
    "    c_inits_bitag = dict()\n",
    "    \n",
    "    for word in corpus:\n",
    "        for i in range(len(word)):\n",
    "            couple= word[i]\n",
    "            letter = couple[0]\n",
    "            correction = couple[1]\n",
    "            #Counting letter \n",
    "            if letter in c_letter:\n",
    "                c_letter[letter] +=1\n",
    "            else:\n",
    "                c_letter[letter] =1  \n",
    "            #Counting correction\n",
    "            if correction in c_correction:\n",
    "                c_correction[correction] +=1\n",
    "            else:\n",
    "                c_correction[correction] =1\n",
    "            #Counting par(letter, correction)\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] +=1\n",
    "            else :\n",
    "                c_pairs[couple] =1\n",
    "            #Counting bitag(corr_i, corr_(i+1))\n",
    "            if i > 0 and i < len(word)-1:\n",
    "                bitag = (word[i-1][1], correction)\n",
    "                if bitag in c_bitag:\n",
    "                    c_bitag[bitag] += 1\n",
    "                else:\n",
    "                    c_bitag[bitag] =1\n",
    "                    \n",
    "            #Counting tritag\n",
    "            if i > 1:\n",
    "                tritag = (word[i-2][1],word[i-1][1], correction)\n",
    "                if tritag in c_tritag :\n",
    "                    c_tritag[tritag] +=1\n",
    "                else :\n",
    "                    c_tritag[tritag] =1\n",
    "                    \n",
    "            if i == 0 and len(word)>1:\n",
    "                if correction in c_inits:\n",
    "                    c_inits[correction] +=1\n",
    "                else :\n",
    "                    c_inits[correction] =1\n",
    "                bg_first=(correction,word[i+1][1])\n",
    "                \n",
    "                if bg_first in c_inits_bitag:\n",
    "                    c_inits_bitag[bg_first]+=1\n",
    "                else:\n",
    "                    c_inits_bitag[bg_first]=1\n",
    "                    \n",
    "    return c_letter, c_correction, c_pairs, c_bitag, c_tritag, c_inits, c_inits_bitag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_counts_each_observation_sequence(observations, predicted_tags):\n",
    "    count_pair = dict()\n",
    "    count_bitag = dict()\n",
    "    count_tritag = dict()\n",
    "    \n",
    "    count_tritag_predicted = dict()\n",
    "    count_pair_predicted = dict()\n",
    "\n",
    "    for i in range(len(observations)):\n",
    "        pair = observations[i]\n",
    "        observation = pair[0]\n",
    "        tag = pair[1]\n",
    "            \n",
    "        #Counting pair(observation, tag)\n",
    "        if pair in count_pair:\n",
    "            count_pair[pair] += 1\n",
    "        else :\n",
    "            count_pair[pair] = 1\n",
    "            \n",
    "\n",
    "        #Counting tritag(tag_(i-2), tag_(i-1), tag_i)\n",
    "        if i > 1:\n",
    "            tritag = (observations[i-2][1],observations[i-1][1], tag)\n",
    "            if tritag in count_tritag :\n",
    "                count_tritag[tritag] += 1\n",
    "            else :\n",
    "                count_tritag[tritag] = 1\n",
    "        \n",
    "        \n",
    "        #Counting predicted pair\n",
    "        predicted_pair = (observation, predicted_tags[i])\n",
    "        \n",
    "        if predicted_pair in count_pair_predicted:\n",
    "            count_pair_predicted[predicted_pair] += 1\n",
    "        else :\n",
    "            count_pair_predicted[predicted_pair] = 1\n",
    "        \n",
    "        \n",
    "        #Counting predicted tritag(tag_(i-2), tag_(i-1), tag_i)\n",
    "        tag_predicted = predicted_tags[i]\n",
    "        if i > 1:\n",
    "            tritag_predicted = (predicted_tags[i-2],predicted_tags[i-1], tag_predicted)\n",
    "            if tritag_predicted in count_tritag_predicted :\n",
    "                count_tritag_predicted[tritag_predicted] += 1\n",
    "            else :\n",
    "                count_tritag_predicted[tritag_predicted] = 1\n",
    "\n",
    "    return count_pair, count_tritag, count_tritag_predicted, count_pair_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals\n",
    "import unicodedata\n",
    "import re \n",
    "from nltk.tag.api import TaggerI\n",
    "\n",
    "try:\n",
    "    import pycrfsuite\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "class CRFTagger(TaggerI):\n",
    "    \"\"\"\n",
    "    A module for POS tagging using CRFSuite https://pypi.python.org/pypi/python-crfsuite\n",
    "    \n",
    "    >>> from nltk.tag import CRFTagger\n",
    "    >>> ct = CRFTagger()\n",
    " \n",
    "    >>> train_data = [[('University','Noun'), ('is','Verb'), ('a','Det'), ('good','Adj'), ('place','Noun')],\n",
    "    ... [('dog','Noun'),('eat','Verb'),('meat','Noun')]]\n",
    "    \n",
    "    >>> ct.train(train_data,'model.crf.tagger')\n",
    "    >>> ct.tag_sents([['dog','is','good'], ['Cat','eat','meat']])\n",
    "    [[('dog', 'Noun'), ('is', 'Verb'), ('good', 'Adj')], [('Cat', 'Noun'), ('eat', 'Verb'), ('meat', 'Noun')]]\n",
    "    \n",
    "    >>> gold_sentences = [[('dog','Noun'),('is','Verb'),('good','Adj')] , [('Cat','Noun'),('eat','Verb'), ('meat','Noun')]] \n",
    "    >>> ct.evaluate(gold_sentences) \n",
    "    1.0\n",
    "    \n",
    "    Setting learned model file  \n",
    "    >>> ct = CRFTagger() \n",
    "    >>> ct.set_model_file('model.crf.tagger')\n",
    "    >>> ct.evaluate(gold_sentences)\n",
    "    1.0\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,  feature_func = None, verbose = False, training_opt = {}):\n",
    "        \"\"\"\n",
    "        Initialize the CRFSuite tagger \n",
    "        :param feature_func: The function that extracts features for each token of a sentence. This function should take \n",
    "        2 parameters: tokens and index which extract features at index position from tokens list. See the build in \n",
    "        _get_features function for more detail.   \n",
    "        :param verbose: output the debugging messages during training.\n",
    "        :type verbose: boolean  \n",
    "        :param training_opt: python-crfsuite training options\n",
    "        :type training_opt : dictionary \n",
    "        \n",
    "        Set of possible training options (using LBFGS training algorithm).  \n",
    "         'feature.minfreq' : The minimum frequency of features.\n",
    "         'feature.possible_states' : Force to generate possible state features.\n",
    "         'feature.possible_transitions' : Force to generate possible transition features.\n",
    "         'c1' : Coefficient for L1 regularization.\n",
    "         'c2' : Coefficient for L2 regularization.\n",
    "         'max_iterations' : The maximum number of iterations for L-BFGS optimization.\n",
    "         'num_memories' : The number of limited memories for approximating the inverse hessian matrix.\n",
    "         'epsilon' : Epsilon for testing the convergence of the objective.\n",
    "         'period' : The duration of iterations to test the stopping criterion.\n",
    "         'delta' : The threshold for the stopping criterion; an L-BFGS iteration stops when the\n",
    "                    improvement of the log likelihood over the last ${period} iterations is no greater than this threshold.\n",
    "         'linesearch' : The line search algorithm used in L-BFGS updates:\n",
    "                           { 'MoreThuente': More and Thuente's method,\n",
    "                              'Backtracking': Backtracking method with regular Wolfe condition,\n",
    "                              'StrongBacktracking': Backtracking method with strong Wolfe condition\n",
    "                           } \n",
    "         'max_linesearch' :  The maximum number of trials for the line search algorithm.\n",
    "         \n",
    "        \"\"\"\n",
    "                   \n",
    "        self._model_file = ''\n",
    "        self._tagger = pycrfsuite.Tagger()\n",
    "        \n",
    "        if feature_func is None:\n",
    "            self._feature_func =  self._get_features\n",
    "        else:\n",
    "            self._feature_func =  feature_func\n",
    "        \n",
    "        self._verbose = verbose \n",
    "        self._training_options = training_opt\n",
    "        self._pattern = re.compile(r'\\d')\n",
    "        \n",
    "    def set_model_file(self, model_file):\n",
    "        self._model_file = model_file\n",
    "        self._tagger.open(self._model_file)\n",
    "\n",
    "            \n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including \n",
    "             - Current Word \n",
    "             - Is Capitalized ?\n",
    "             - Has Punctuation ?\n",
    "             - Has Number ?\n",
    "             - Suffixes up to length 3\n",
    "        Note that : we might include feature over previous word, next word ect. \n",
    "        \n",
    "        :return : a list which contains the features\n",
    "        :rtype : list(str)    \n",
    "        \n",
    "        \"\"\" \n",
    "        token = tokens[idx]\n",
    "        \n",
    "        feature_list = []\n",
    "        \n",
    "        if not token:\n",
    "            return feature_list\n",
    "        \n",
    "# Remove for typos correction            \n",
    "#         # Capitalization \n",
    "#         if token[0].isupper():\n",
    "#             feature_list.append('CAPITALIZATION')\n",
    "        \n",
    "#         # Number \n",
    "#         if re.search(self._pattern, token) is not None:\n",
    "#             feature_list.append('HAS_NUM') \n",
    "        \n",
    "#         # Punctuation\n",
    "#         punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n",
    "#         if all (unicodedata.category(x) in punc_cat for x in token):\n",
    "#             feature_list.append('PUNCTUATION')\n",
    "        \n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append('SUF_' + token[-1:]) \n",
    "        if len(token) > 2: \n",
    "            feature_list.append('SUF_' + token[-2:])    \n",
    "        if len(token) > 3: \n",
    "            feature_list.append('SUF_' + token[-3:])\n",
    "            \n",
    "        feature_list.append('WORD_' + token )\n",
    "        \n",
    "        return feature_list\n",
    "        \n",
    "    def tag_sents(self, sents):\n",
    "        '''\n",
    "        Tag a list of sentences. NB before using this function, user should specify the mode_file either by \n",
    "                       - Train a new model using ``train'' function \n",
    "                       - Use the pre-trained model which is set via ``set_model_file'' function  \n",
    "        :params sentences : list of sentences needed to tag. \n",
    "        :type sentences : list(list(str))\n",
    "        :return : list of tagged sentences. \n",
    "        :rtype : list (list (tuple(str,str))) \n",
    "        '''\n",
    "        if self._model_file == '':\n",
    "            raise Exception(' No model file is found !! Please use train or set_model_file function')\n",
    "        \n",
    "        # We need the list of sentences instead of the list generator for matching the input and output\n",
    "        result = []  \n",
    "        for tokens in sents:\n",
    "            features = [self._feature_func(tokens,i) for i in range(len(tokens))]\n",
    "            labels = self._tagger.tag(features)\n",
    "                \n",
    "            if len(labels) != len(tokens):\n",
    "                raise Exception(' Predicted Length Not Matched, Expect Errors !')\n",
    "            \n",
    "            tagged_sent = list(zip(tokens,labels))\n",
    "            result.append(tagged_sent)\n",
    "            \n",
    "        return result \n",
    "\n",
    "    \n",
    "    def train(self, train_data, model_file):\n",
    "        '''\n",
    "        Train the CRF tagger using CRFSuite  \n",
    "        :params train_data : is the list of annotated sentences.        \n",
    "        :type train_data : list (list(tuple(str,str)))\n",
    "        :params model_file : the model will be saved to this file.     \n",
    "         \n",
    "        '''\n",
    "        trainer = pycrfsuite.Trainer(verbose=self._verbose)\n",
    "        if \"algorithm\" in self._training_options:\n",
    "            trainer.select(self._training_options[\"algorithm\"])\n",
    "        #print trainer.get_params()\n",
    "        #trainer.set_params(self._training_options)\n",
    "        \n",
    "        for sent in train_data:\n",
    "            tokens,labels = zip(*sent)\n",
    "            features = [self._feature_func(tokens,i) for i in range(len(tokens))]\n",
    "            trainer.append(features,labels)\n",
    "                        \n",
    "        # Now train the model, the output should be model_file\n",
    "        trainer.train(model_file)\n",
    "        # Save the model file\n",
    "        self.set_model_file(model_file) \n",
    "\n",
    "\n",
    "    def tag(self, tokens):\n",
    "        '''\n",
    "        Tag a sentence using Python CRFSuite Tagger. NB before using this function, user should specify the mode_file either by \n",
    "                       - Train a new model using ``train'' function \n",
    "                       - Use the pre-trained model which is set via ``set_model_file'' function  \n",
    "        :params tokens : list of tokens needed to tag. \n",
    "        :type tokens : list(str)\n",
    "        :return : list of tagged tokens. \n",
    "        :rtype : list (tuple(str,str)) \n",
    "        '''\n",
    "        \n",
    "        return self.tag_sents([tokens])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(correction_words,true_words):\n",
    "    \"\"\"Compares the corrections and true_vals\"\"\"\n",
    "    error=0\n",
    "    total=0\n",
    "    for f, b in zip(correction_words, true_words):\n",
    "        if cmp(f,b)!=0:\n",
    "            for i in range(len(f)):\n",
    "                if f[i]!=b[i]:\n",
    "                    error+=1\n",
    "        total+=len(f)\n",
    "\n",
    "    return float(error)/float(total)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and making counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "train10=pickle.load( open( \"typos-data/train10.pkl\", \"rb\" ))\n",
    "test10=pickle.load( open( \"typos-data/test10.pkl\", \"rb\" ))\n",
    "train20=pickle.load( open( \"typos-data/train20.pkl\", \"rb\" ))\n",
    "test20=pickle.load( open( \"typos-data/test20.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_letter, c_correction, c_pairs, c_bitag, c_tritag, c_inits, c_inits_bitag=make_counts(train10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the HMM and supervised training ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=c_correction.keys(), observation_list=c_letter.keys(),\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None)\n",
    "hmm.supervised_training_ME( c_pairs, c_bitag, c_tritag ,c_inits, c_inits_bitag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing HMM with ML parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_words=[]\n",
    "true_words=[] #denotes all underlying hidden states\n",
    "for sent in test10:\n",
    "    data_test = np.asarray(sent)\n",
    "    obs,states = np.hsplit(data_test,2)\n",
    "    wrong_words.append(obs.tostring())\n",
    "    true_words.append(list(states.tostring()))\n",
    "wrong_words = np.array(wrong_words)\n",
    "true_words = np.array(true_words)   #These are the true lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6693510f9610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwrong_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-f367ea6bfdf4>\u001b[0m in \u001b[0;36mviterbi\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    188\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_u\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_v\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                                                      self.observation_proba[letter,i_v],w) \n\u001b[1;32m--> 190\u001b[1;33m                                                     for w in self.get_possible_corrections(k-2)])                       \n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                         \u001b[0mtemp_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackpointer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correction_words=[]\n",
    "for word in wrong_words:\n",
    "    if(len(word)>1):\n",
    "        p,v=hmm.viterbi(word)\n",
    "    else:\n",
    "        v=list(word)\n",
    "    correction_words.append(v)\n",
    "correction_words=np.array(correction_words)\n",
    "\n",
    "print compute_error(correction_words,true_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training and testing HMM with Perceptron parameter estimates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-2a4aba6dae97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupervised_training_perceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-f367ea6bfdf4>\u001b[0m in \u001b[0;36msupervised_training_perceptron\u001b[1;34m(self, T, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mobservations\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                         \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m                         \u001b[0mcount_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_tritag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_tritag_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_pair_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_counts_each_observation_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-f367ea6bfdf4>\u001b[0m in \u001b[0;36mviterbi\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    188\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition_proba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_u\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi_v\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                                                      self.observation_proba[letter,i_v],w) \n\u001b[1;32m--> 190\u001b[1;33m                                                     for w in self.get_possible_corrections(k-2)])                       \n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                         \u001b[0mtemp_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbackpointer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm.supervised_training_perceptron(1,train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99043715847\n"
     ]
    }
   ],
   "source": [
    "correction_words=[]\n",
    "for word in wrong_words:\n",
    "    if(len(word)>1):\n",
    "        p,v=hmm.viterbi(word)\n",
    "    else:\n",
    "        v=list(word)\n",
    "    correction_words.append(v)\n",
    "correction_words=np.array(correction_words)\n",
    "\n",
    "print compute_error(correction_words,true_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature.minfreq', 'feature.possible_states', 'feature.possible_transitions', 'max_iterations', 'epsilon']\n",
      "0.201912568306\n"
     ]
    }
   ],
   "source": [
    "#Modify original code: Remove sentence modification (Punctuation) -> Typos Correction\n",
    "\n",
    "# Download: pip install python-crfsuite\n",
    "\n",
    "ct = CRFTagger() #For default ML-estimation\n",
    "#ct = CRFTagger(training_opt={\"algorithm\":\"ap\"}) #For changing the training method to perceptron\n",
    "ct.train(train10,'model.crf.tagger')\n",
    "\n",
    "correction_words_new=[]\n",
    "for word in wrong_words:\n",
    "    v = ct.tag_sents(list(word))\n",
    "    correct = []\n",
    "    for c in v:\n",
    "        correct.append(c[0][1])\n",
    "        \n",
    "    correction_words_new.append(correct)\n",
    "    \n",
    "print compute_error(correction_words_new,true_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxEnt model with Perceptron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0520491803279\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.train(train10)\n",
    "correction_words_new=[]\n",
    "\n",
    "for word in wrong_words:\n",
    "    v = tagger.tag(list(word))\n",
    "    correct = []\n",
    "    for c in v:\n",
    "        correct.append(c[1])\n",
    "    correction_words_new.append(correct)\n",
    "    \n",
    "print compute_error(correction_words_new,true_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
